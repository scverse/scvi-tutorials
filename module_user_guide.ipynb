{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MfvyiShHsS8"
   },
   "source": [
    "# Constructing a probabilistic module\n",
    "\n",
    "In our previous tutorial, we went over the principle of a dataloader, and how scvi-tools interacts natively with anndata. In this tutorial, we will go in further details towards the creation of a novel statistical method for single-cell omics data. The gist of a method is essentially summarized in two components:\n",
    "\n",
    "1. **A generative model**, that aims at efficiently mimicking the underlying data distribution. Ideally, a generative model fits the data well, while making use of informative and interpretable latent variables that summarize certain aspects of the data. For example, scVI aims at summarizing the biological signal of the gene expression $x_{n}$ of a cell $n$ into a latent variable $z_n$ that represents cell-specific transcriptomic states.\n",
    "2. An **inference method**, that aims at \"guessing\" the latent variables (and/or the parameters of the model), based on the data. \n",
    "\n",
    "scvi-tools proposes two different backends for the developement of such probabilistic programs, both being built on top of PyTorch Lightning:\n",
    "\n",
    "1. A **vanilla PyTorch** interface, on which most of the models are implemented. We expect this interface to be useful when the objective function (likelihood or evidence lower bound) may be easily written in pytorch, or when ad-hoc stochastic variational inference procedures are needed.\n",
    "2. A **Pyro** interface, which we expect to be useful when working with large hierarchical Bayesian models or when the inference used relies on a clear algorithmic recipe (ADVI, VAEs).\n",
    "\n",
    "In this tutorial, we shall present how to create a new Module to reimplement a simple version of scVI from scratch. We will proceed as follow:\n",
    "\n",
    "1. Brief introduction to the scVI model and its inference recipe\n",
    "2. Drafting the inner components (neural nets and likelihood functions) \n",
    "3. Crafting the Module in vanilla PyTorch\n",
    "4. Crafting the Module in Pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C22Q0uVkg4pO"
   },
   "source": [
    "## A simpler scVI model and its inference recipe\n",
    "\n",
    "We work here with a simpler version of scVI to highlight how easy it is to craft new modules inside of scvi-tools. \n",
    "\n",
    "### The generative model\n",
    "\n",
    "Let \n",
    "\n",
    "$$z_n \\sim \\textrm{Normal}(0, I),$$\n",
    "\n",
    "be a latent random vector representing the transcriptomic state of cell $n$, tyipically low-dimensional (e.g., dimension 10). Let $l_{n}$ be the number of captured unique molecule identifier in cell $n$, that we assume to be an observed random variable. The gene expression of a gene $g$ in a cell $n$, $x_{ng}$ is obtained as: \n",
    "\n",
    "$$x_{ng} \\sim \\textrm{NegativeBinomial}\\left(l_n f^g(z_n), \\theta_g\\right),$$\n",
    "\n",
    "where $f$ is a function mapping the latent space to the simplex of the gene expression space. $\\theta_g$ is a positive parameter to be learned: the dispersion parameter of the negative binomial distribution. \n",
    "\n",
    "### The inference mechanism\n",
    "\n",
    "The scvi-tools codebase is expected to work with many different inference mecanisms (e.g., AEVB, VI, EM, MAP, MLE, etc.). In this precise tutorial, we focus on auto-encoding variational Bayes. AEVB is part of the family of variational inference recipes, in which one seeks to maximize a lower-bound of the likelihood (when the likelihood itself is intractable). \n",
    "\n",
    "In our case, we aim at finding the parameters $\\Theta = \\{\\theta, f\\}$ that maximize the log-likelihood of the data $\\log p_\\Theta(x)$ (we identify the function $f$ to its parameters). As the likelihood is intractable, we optimize instead a lower-bound:\n",
    "\n",
    "$$ \\log p_\\Theta(x) \\geq \\mathbb{E}_{q(z \\mid x)}\\log p_\\Theta(x \\mid z) - \\textrm{KL}\\left(q(z \\mid x) \\mid p(z)\\right), $$\n",
    "\n",
    "in which the distribution $q(z \\mid x)$ is named the variational distribution.\n",
    "\n",
    "There are two important things to mention. First, we must specify a parameterization for the variational distribution. In AEVB, $q(z \\mid x)$ is specified via a pair of neural networks:\n",
    "\n",
    "$$q(z \\mid x) \\sim \\textrm{Normal}\\left(g_\\mu(x), \\textrm{diag}(g^2_\\sigma(x))\\right).$$\n",
    "\n",
    "Second, optimizing the lower bound cannot be done in closed-form. Instead, gradients are approximated by sampling from the variational distribution using the reparameterization trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiSEraJgfJTo"
   },
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "Note \n",
    "\n",
    "A great advantage of the Pyro API is that only the specification of the parameterization of the inference networks is required (this is called the guide), while our vanilla pytorch API requires the implementation of the reparameterization trick, as well as the evidence lower bound. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeI1HAhnfLED"
   },
   "source": [
    "Great, now let's start coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet scvi-colab\n",
    "from scvi_colab import install\n",
    "\n",
    "install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gpAEbZ9UgCp7"
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from scvi import REGISTRY_KEYS\n",
    "from scvi.module.base import (\n",
    "    BaseModuleClass,\n",
    "    LossRecorder,\n",
    "    PyroBaseModuleClass,\n",
    "    auto_move_data,\n",
    ")\n",
    "from torch.distributions import NegativeBinomial, Normal\n",
    "from torch.distributions import kl_divergence as kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlNCMtDSVR10"
   },
   "source": [
    "## Drafting the inner components (neural nets) \n",
    "\n",
    "We aim here at creating all the elementary stochastic computation units needed to describe the generative model, as well as performing inference. We will then craft those units together into a Module (either in vanilla Pytorch or in Pyro).\n",
    "\n",
    "Our model includes one neural network, that we will refer to as the decoder. Our inference recipe includes two neural networks (the encoders). Because each neural network will have a specific output non-linearity to handle the different cases, we will create a generic class. The class inherits from the torch.nn.Module class so that it's parameters are automatically optimized during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tc7bIj_EZOE7"
   },
   "outputs": [],
   "source": [
    "class MyNeuralNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        link_var: Literal[\"exp\", \"none\", \"softmax\"],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Encodes data of ``n_input`` dimensions into a space of ``n_output`` dimensions.\n",
    "\n",
    "        Uses a one layer fully-connected neural network with 128 hidden nodes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_input\n",
    "            The dimensionality of the input\n",
    "        n_output\n",
    "            The dimensionality of the output\n",
    "        link_var\n",
    "            The final non-linearity\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.neural_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_input, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, n_output),\n",
    "        )\n",
    "        self.transformation = None\n",
    "        if link_var == \"softmax\":\n",
    "            self.transformation = torch.nn.Softmax(dim=-1)\n",
    "        elif link_var == \"exp\":\n",
    "            self.transformation = torch.exp\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        output = self.neural_net(x)\n",
    "        if self.transformation:\n",
    "            output = self.transformation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfCdXp2LgpLc"
   },
   "source": [
    "We can instantiate and immediately test out this elementary unit, that may be used for any of the three neural networks needed for our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFgRpyYTgn1C",
    "outputId": "541cf4b3-3ebc-4b55-ebc9-2fbe2b4be21b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNeuralNet(\n",
       "  (neural_net): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (transformation): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the module and observe the architecture. scvi-tools contains many modules to automatically handle complex covariates!\n",
    "my_neural_net = MyNeuralNet(100, 10, \"softmax\")\n",
    "my_neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Em-wJpoBhiPN",
    "outputId": "e3d0fc58-0916-45fc-c716-5d6a6097cd04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1203, 0.0992, 0.1047, 0.0845, 0.1103, 0.1159, 0.1152, 0.0656, 0.1012,\n",
       "         0.0830],\n",
       "        [0.1076, 0.0793, 0.1483, 0.0515, 0.0989, 0.0877, 0.1139, 0.0826, 0.0830,\n",
       "         0.1473]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe that the output sums to 1 and are positive!\n",
    "x = torch.randn((2, 100))\n",
    "my_neural_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq9MsZw-VT4u"
   },
   "source": [
    "## Crafting the Module in vanilla PyTorch\n",
    "\n",
    "All of our vanilla Pytorch modules inherit from the `BaseModuleClass`, and must implement the following methods:\n",
    "\n",
    "1. `_get_generative_input()`: selecting the registered tensors from the anndata, as well as the latent variables (from inference) used in the model\n",
    "2. `generative()`: mapping the generative inputs to the parameters of the data likelihood function\n",
    "1. `_get_inference_input()`: selecting the registered tensors from the anndata used in the inference\n",
    "2. `inference()`: mapping the inference inputs to the parameters of the variational distribution\n",
    "3. `loss()`: the log-likelihood or its lower bound\n",
    "4. `sample()` [Optional]: this signature may be used to sample new datapoints (prior predictive or posterior predictive), outside of this tutorial's topic\n",
    "\n",
    "The `BaseModuleClass` has already implemented a `.forward()` method that will be used by the trainer. The schematic of the method is as follows:\n",
    "\n",
    "1. Get the tensors for inference, and feed them through the `inference` method to recover the latent variables\n",
    "2. Get the tensors used for describing the generative model, and feed the through the `generative` method to recover the parameters of the data likelihood\n",
    "3. Evaluate the data fit in the `loss` method.\n",
    "\n",
    "The trainer then calculates the gradients of the loss with respect to the model and the inference parameters to update them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9_jmZX0kYd5u"
   },
   "outputs": [],
   "source": [
    "class MyModule(BaseModuleClass):\n",
    "    \"\"\"\n",
    "    Skeleton Variational auto-encoder model.\n",
    "\n",
    "    Here we implement a basic version of scVI's underlying VAE [Lopez18]_.\n",
    "    This implementation is for instructional purposes only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input genes\n",
    "    n_latent\n",
    "        Dimensionality of the latent space\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_latent: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # in the init, we create the parameters of our elementary stochastic computation unit.\n",
    "\n",
    "        # First, we setup the parameters of the generative model\n",
    "        self.decoder = MyNeuralNet(n_latent, n_input, \"softmax\")\n",
    "        self.log_theta = torch.nn.Parameter(torch.randn(n_input))\n",
    "\n",
    "        # Second, we setup the parameters of the variational distribution\n",
    "        self.mean_encoder = MyNeuralNet(n_input, n_latent, \"none\")\n",
    "        self.var_encoder = MyNeuralNet(n_input, n_latent, \"exp\")\n",
    "\n",
    "    def _get_inference_input(self, tensors):\n",
    "        \"\"\"Parse the dictionary to get appropriate args\"\"\"\n",
    "        # let us fetch the raw counts, and add them to the dictionary\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]\n",
    "\n",
    "        input_dict = dict(x=x)\n",
    "        return input_dict\n",
    "\n",
    "    @auto_move_data\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        High level inference method.\n",
    "\n",
    "        Runs the inference (encoder) model.\n",
    "        \"\"\"\n",
    "        # log the input to the variational distribution for numerical stability\n",
    "        x_ = torch.log(1 + x)\n",
    "        # get variational parameters via the encoder networks\n",
    "        qz_m = self.mean_encoder(x_)\n",
    "        qz_v = self.var_encoder(x_)\n",
    "        # get one sample to feed to the generative model\n",
    "        # under the hood here is the Reparametrization trick (Rsample)\n",
    "        z = Normal(qz_m, torch.sqrt(qz_v)).rsample()\n",
    "\n",
    "        outputs = dict(qz_m=qz_m, qz_v=qz_v, z=z)\n",
    "        return outputs\n",
    "\n",
    "    def _get_generative_input(self, tensors, inference_outputs):\n",
    "        z = inference_outputs[\"z\"]\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        # here we extract the number of UMIs per cell as a known quantity\n",
    "        library = torch.sum(x, dim=1, keepdim=True)\n",
    "\n",
    "        input_dict = {\n",
    "            \"z\": z,\n",
    "            \"library\": library,\n",
    "        }\n",
    "        return input_dict\n",
    "\n",
    "    @auto_move_data\n",
    "    def generative(self, z, library):\n",
    "        \"\"\"Runs the generative model.\"\"\"\n",
    "\n",
    "        # get the \"normalized\" mean of the negative binomial\n",
    "        px_scale = self.decoder(z)\n",
    "        # get the mean of the negative binomial\n",
    "        px_rate = library * px_scale\n",
    "        # get the dispersion parameter\n",
    "        theta = torch.exp(self.log_theta)\n",
    "\n",
    "        return dict(px_scale=px_scale, theta=theta, px_rate=px_rate)\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        tensors,\n",
    "        inference_outputs,\n",
    "        generative_outputs,\n",
    "    ):\n",
    "\n",
    "        # here, we would like to form the ELBO. There are two terms:\n",
    "        #   1. one that pertains to the likelihood of the data\n",
    "        #   2. one that pertains to the variational distribution\n",
    "        # so we extract all the required information\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        px_rate = generative_outputs[\"px_rate\"]\n",
    "        theta = generative_outputs[\"theta\"]\n",
    "        qz_m = inference_outputs[\"qz_m\"]\n",
    "        qz_v = inference_outputs[\"qz_v\"]\n",
    "\n",
    "        # term 1\n",
    "        # the pytorch NB distribution uses a different parameterization\n",
    "        # so we must apply a quick transformation (included in scvi-tools, but here we use the pytorch code)\n",
    "        nb_logits = (px_rate + 1e-4).log() - (theta + 1e-4).log()\n",
    "        log_lik = (\n",
    "            NegativeBinomial(total_count=theta, logits=nb_logits)\n",
    "            .log_prob(x)\n",
    "            .sum(dim=-1)\n",
    "        )\n",
    "\n",
    "        # term 2\n",
    "        prior_dist = Normal(torch.zeros_like(qz_m), torch.ones_like(qz_v))\n",
    "        var_post_dist = Normal(qz_m, torch.sqrt(qz_v))\n",
    "        kl_divergence = kl(var_post_dist, prior_dist).sum(dim=1)\n",
    "\n",
    "        elbo = log_lik - kl_divergence\n",
    "        loss = torch.mean(-elbo)\n",
    "        return LossRecorder(loss, -log_lik, kl_divergence, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8_g2WMP_UR5",
    "outputId": "35103cd2-55db-4c98-f3f9-4cc7b8007a7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModule(\n",
       "  (decoder): MyNeuralNet(\n",
       "    (neural_net): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=100, bias=True)\n",
       "    )\n",
       "    (transformation): Softmax(dim=-1)\n",
       "  )\n",
       "  (mean_encoder): MyNeuralNet(\n",
       "    (neural_net): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (var_encoder): MyNeuralNet(\n",
       "    (neural_net): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try creating a module and see the description:\n",
    "MyModule(100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV6ubmEUVWwB"
   },
   "source": [
    "## Crafting the Module in Pyro\n",
    "\n",
    "The Pyro Module has less code, as it is not needed to write the evidence lower bound. Still, one must implement the following to inherit from our `PyroModuleClass`:\n",
    "\n",
    "1. A static method `_get_fn_args_from_batch()`: a function that extracts the necessary tensors to be sent to the generative model and the inference (called a guide in Pyro). In the Pyro case, both functions must have the same signature.\n",
    "2. A `model()` method: that simulates the data generating process using the Pyro syntax.\n",
    "3. A `guide()` method: that explicitly tells Pyro how to perform inference. Pyro has some automatic guides in the context of ADVI, but for AEVB we will write our own guide with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Atg304-AVUaP"
   },
   "outputs": [],
   "source": [
    "class MyPyroModule(PyroBaseModuleClass):\n",
    "    def __init__(self, n_input, n_latent):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.n_input = n_input\n",
    "        # in the init, we create the parameters of our elementary stochastic computation unit.\n",
    "\n",
    "        # First, we setup the parameters of the generative model\n",
    "        self.decoder = MyNeuralNet(n_latent, n_input, \"softmax\")\n",
    "        self.log_theta = torch.nn.Parameter(torch.randn(n_input))\n",
    "\n",
    "        # Second, we setup the parameters of the variational distribution\n",
    "        self.mean_encoder = MyNeuralNet(n_input, n_latent, \"none\")\n",
    "        self.var_encoder = MyNeuralNet(n_input, n_latent, \"exp\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_fn_args_from_batch(tensor_dict):\n",
    "        x = tensor_dict[REGISTRY_KEYS.X_KEY]\n",
    "        library = torch.sum(x, dim=1, keepdim=True)\n",
    "        return (x, library), {}\n",
    "\n",
    "    def model(self, x, library):\n",
    "        # register PyTorch module `decoder` with Pyro\n",
    "        pyro.module(\"scvi\", self)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # setup hyperparameters for prior p(z)\n",
    "            z_loc = x.new_zeros(torch.Size((x.shape[0], self.n_latent)))\n",
    "            z_scale = x.new_ones(torch.Size((x.shape[0], self.n_latent)))\n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            # get the \"normalized\" mean of the negative binomial\n",
    "            px_scale = self.decoder(z)\n",
    "            # get the mean of the negative binomial\n",
    "            px_rate = library * px_scale\n",
    "            # get the dispersion parameter\n",
    "            theta = torch.exp(self.log_theta)\n",
    "            # build count distribution\n",
    "            nb_logits = (px_rate + 1e-4).log() - (theta + 1e-4).log()\n",
    "            x_dist = dist.NegativeBinomial(total_count=theta, logits=nb_logits)\n",
    "            # score against actual counts\n",
    "            pyro.sample(\"obs\", x_dist.to_event(1), obs=x)\n",
    "\n",
    "    def guide(self, x, log_library):\n",
    "        # define the guide (i.e. variational distribution) q(z|x)\n",
    "        pyro.module(\"scvi\", self)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            x_ = torch.log(1 + x)\n",
    "            qz_m = self.mean_encoder(x_)\n",
    "            qz_v = self.var_encoder(x_)\n",
    "            # sample the latent code z\n",
    "            pyro.sample(\"latent\", dist.Normal(qz_m, torch.sqrt(qz_v)).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJkrfg77AZ6B",
    "outputId": "168a123a-10e1-4054-87e5-603ca50143f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyPyroModule(\n",
       "  (decoder): MyNeuralNet(\n",
       "    (neural_net): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "    )\n",
       "    (transformation): Softmax(dim=-1)\n",
       "  )\n",
       "  (mean_encoder): MyNeuralNet(\n",
       "    (neural_net): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (var_encoder): MyNeuralNet(\n",
       "    (neural_net): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try creating an object\n",
    "MyPyroModule(10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NtE1sVoAZk-"
   },
   "source": [
    "Let's sum up what we learned:\n",
    "\n",
    "1. How does the Module work in scvi-tools\n",
    "2. How is Pyro integrated into scvi-tools\n",
    "\n",
    "In the next tutorial, you will learn how to train these modules, by wrapping them into a Model! "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "module_user_guide.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
