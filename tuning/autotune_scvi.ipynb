{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model hyperparameter tuning with scVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "`scvi.autotune` development is still in progress. The API is subject to change.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding an effective set of model hyperparameters (e.g. learning rate, number of hidden layers, etc.) is an important component in training a model as its performance can be highly dependent on these non-trainable parameters. Manually tuning a model often involves picking a set of hyperparameters to search over and then evaluating different configurations over a validation set for a desired metric. This process can be time consuming and can require some prior intuition about a model and dataset pair, which is not always feasible.\n",
    "\n",
    "In this tutorial, we show how to use `scvi`'s [`autotune`](https://docs.scvi-tools.org/en/latest/api/user.html#model-hyperparameter-autotuning) module, which allows us to automatically find a good set of model hyperparameters using [Ray Tune](https://docs.ray.io/en/latest/tune/index.html). We will use `SCVI` and a subsample of the [heart cell atlas](https://www.heartcellatlas.org/#DataSources) for the task of batch correction, but the principles outlined here can be applied to any model and dataset. In particular, we will go through the following steps:\n",
    "\n",
    "1. Installing required packages\n",
    "1. Loading and preprocessing the dataset\n",
    "1. Defining the tuner and discovering hyperparameters\n",
    "1. Running the tuner\n",
    "1. Comparing latent spaces\n",
    "1. Optional: Monitoring progress with Tensorboard\n",
    "1. Optional: Tuning over integration metrics with `scib-metrics`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following lines in Google Colab in order to install `scvi-tools`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet scvi-colab\n",
    "# from scvi_colab import install\n",
    "\n",
    "# install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import ray\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import torch\n",
    "from ray import tune\n",
    "from scvi import autotune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scvi.settings.seed = 0\n",
    "print(\"Last run with scvi-tools version:\", scvi.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify `save_dir` below to change where the data files for this tutorial are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(figsize=(4, 4), frameon=False)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "save_dir = tempfile.TemporaryDirectory()\n",
    "scvi.settings.logging_dir = save_dir.name\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs={\"facecolor\" : \"w\"}\n",
    "%config InlineBackend.figure_format=\"retina\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = scvi.data.heart_cell_atlas_subsampled(save_path=save_dir.name)\n",
    "adata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only preprocessing step we will perform in this case will be to subsample the dataset for 2000 highly variable genes using `scanpy` for faster model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(adata, n_top_genes=2000, flavor=\"seurat_v3\", subset=True)\n",
    "adata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the tuner and discovering hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of our workflow is the same as the standard `scvi-tools` workflow: we start with our desired model class, and we register our dataset with it using `setup_anndata`. All datasets must be registered prior to hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = scvi.model.SCVI\n",
    "model_cls.setup_anndata(adata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main entry point to the `autotune` module is the `ModelTuner` class, a wrapper around [`ray.tune.Tuner`](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tuner) with additional functionality specific to `scvi-tools`. We can define a new `ModelTuner` by providing it with our model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scvi_tuner = autotune.ModelTuner(model_cls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelTuner` will register all tunable hyperparameters in `SCVI` -- these can be viewed by calling `info()`. By default, this method will display three tables:\n",
    "\n",
    "1. **Tunable hyperparameters**: The names of hyperparameters that can be tuned, their default values, and the internal classes they are defined in.\n",
    "1. **Available metrics**: The metrics that can be used to evaluate the performance of the model. One of these must be provided when running the tuner.\n",
    "1. **Default search space**: The default search space for the model class, which will be used if no search space is provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scvi_tuner.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the tuner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what hyperparameters are available to us, we can define a search space using the [search space API](https://docs.ray.io/en/latest/tune/api/search_space.html) in `ray.tune`. For this tutorial, we choose a simple search space with two model hyperparameters and one training plan hyperparameter. These can all be combined into a single dictionary that we pass into the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"n_hidden\": tune.choice([64, 128, 256]),\n",
    "    \"n_layers\": tune.choice([1, 2, 3]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple more arguments we should be aware of before fitting the tuner:\n",
    "\n",
    "- `num_samples`: The total number of hyperparameter sets to sample from `search_space`. This is the total number of models that will be trained.\n",
    "\n",
    "  For example, if we set `num_samples=2`, we might sample two models with the following hyperparameter configurations:\n",
    "\n",
    "  ```python\n",
    "  model1 = {\n",
    "      \"n_hidden\": 64,\n",
    "      \"n_layers\": 1,\n",
    "      \"lr\": 0.001,\n",
    "  }\n",
    "  model2 = {\n",
    "      \"n_hidden\": 64,\n",
    "      \"n_layers\": 3,\n",
    "      \"lr\": 0.0001,\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- `max_epochs`: The maximum number of epochs to train each model for.\n",
    "\n",
    "  Note: This does not mean that each model will be trained for `max_epochs`. Depending on the scheduler used, some trials are likely to be stopped early.\n",
    "\n",
    "- `resources`: A dictionary of maximum resources to allocate for the whole experiment. This allows us to run concurrent trials on limited hardware.\n",
    "\n",
    "Now, we can call `fit` on the tuner to start the hyperparameter sweep. This will return a `TuneAnalysis` dataclass, which will contain the best set of hyperparameters, as well as other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(log_to_driver=False)\n",
    "results = scvi_tuner.fit(\n",
    "    adata,\n",
    "    metric=\"validation_loss\",\n",
    "    search_space=search_space,\n",
    "    num_samples=5,\n",
    "    max_epochs=100,\n",
    "    resources={\"cpu\": 10, \"gpu\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.model_kwargs)\n",
    "print(results.train_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing latent spaces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in progress: please check back in the next release!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Monitoring progress with Tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in progress: please check back in the next release!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Tuning over integration metrics with `scib-metrics`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in progress: please check back in the next release!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f978838050607ec9770689d8200902a4128a2ce208b502e911dd714d57e924e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
